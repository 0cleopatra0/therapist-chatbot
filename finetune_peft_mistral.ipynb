{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5W+SKwDnXKoGYbJKzvxCU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0cleopatra0/therapist-chatbot/blob/main/finetune_peft_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asCLvEDYoRix"
      },
      "outputs": [],
      "source": [
        "# 0. Runtime ▸ Change runtime type ▸ GPU, PyTorch 2.x\n",
        "!pip install -q transformers peft datasets bitsandbytes accelerate einops\n",
        "\n",
        "BASE_MODEL   = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "DATA_PATH    = \"/content/mental_health_chat.csv\"      # upload via Colab sidebar\n",
        "NEW_REPO     = \"your-huggingface-username/therapist-adapter\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch, pandas as pd\n",
        "\n",
        "# 1. Load & tokenize\n",
        "df   = pd.read_csv(DATA_PATH)\n",
        "data = load_dataset(\"csv\", data_files=DATA_PATH)[\"train\"]\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "def tok_fn(row):\n",
        "    prompt = f\"<s>[INST]@Therapist. {row['user_input']} [/INST]\"\n",
        "    row[\"input_ids\"] = tok(prompt, truncation=True, max_length=512).input_ids\n",
        "    row[\"labels\"]    = tok(row[\"therapist_response\"], truncation=True,\n",
        "                           max_length=512).input_ids\n",
        "    return row\n",
        "data = data.map(tok_fn)\n",
        "\n",
        "# 2. 4-bit load + LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# 3. Train\n",
        "from transformers import TrainingArguments, Trainer\n",
        "args = TrainingArguments(\n",
        "    output_dir         = \"./peft-out\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    learning_rate      = 2e-4,\n",
        "    num_train_epochs   = 1,\n",
        "    fp16               = True,\n",
        "    logging_steps      = 25,\n",
        "    save_total_limit   = 1\n",
        ")\n",
        "trainer = Trainer(model=model, args=args, train_dataset=data)\n",
        "trainer.train()\n",
        "\n",
        "# 4. Push adapter (tiny, ≈90 MB) to HF Hub\n",
        "model.push_to_hub(NEW_REPO, use_temp_dir=True)\n",
        "tok.push_to_hub(NEW_REPO, use_temp_dir=True)\n",
        "print(\"Adapter pushed to\", NEW_REPO)\n"
      ]
    }
  ]
}