{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e61531",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/datasets\n",
    "!rm -rf ~/.cache/huggingface/hub\n",
    "!pip install --upgrade --quiet datasets transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3f1ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the EmpatheticDialogues dataset\n",
    "dataset = load_dataset(\"empathetic_dialogues\")\n",
    "\n",
    "# Train/test split\n",
    "train_ds = dataset[\"train\"]\n",
    "eval_ds = dataset[\"validation\"] if \"validation\" in dataset else dataset[\"test\"]\n",
    "\n",
    "# Load DialoGPT\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Fix missing pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138cbd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts = [\n",
    "        context + tokenizer.eos_token + response + tokenizer.eos_token\n",
    "        for context, response in zip(examples[\"context\"], examples[\"utterance\"])\n",
    "    ]\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d1c4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_ds = eval_ds.map(preprocess_function, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "train_ds.set_format(type=\"torch\")\n",
    "eval_ds.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d89c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=1,      # Very small batch size\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,      # Accumulate to simulate batch size 8\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,                          # Mixed precision if using GPU\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    report_to=[]                        # Disable W&B to reduce overhead\n",
    ")\n",
    "\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa348a92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f6926",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./dialoGPT_mental_health\")\n",
    "tokenizer.save_pretrained(\"./dialoGPT_mental_health\")\n",
    "\n",
    "# Optional: zip and download\n",
    "!zip -r dialoGPT_mental_health.zip dialoGPT_mental_health\n",
    "from google.colab import files\n",
    "files.download(\"dialoGPT_mental_health.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
